---
title: "Event Streaming"
description: "How to consume real-time events from DroidAgent execution."
---

## Overview

Droidrun provides **real-time event streaming** that gives you visibility into agent execution as it happens. This allows you to build UIs, logging systems, or monitoring tools that react to agent actions in real-time.

Under the hood, Droidrun uses [llama-index workflows](https://docs.llamaindex.ai/en/stable/understanding/workflows/) - an event-driven orchestration system that powers the agent architecture.

## Basic Usage

```python
from droidrun.agent.droid import DroidAgent

# Create and run agent
agent = DroidAgent(goal="Open Gmail and check inbox", config=config)
handler = agent.run()

# Stream events in real-time
async for event in handler.stream_events():
    if isinstance(event, ManagerPlanDetailsEvent):
        print(f"üìã Plan: {event.plan}")
        print(f"üéØ Current subgoal: {event.current_subgoal}")

    elif isinstance(event, ExecutorActionEvent):
        print(f"‚ö° Action: {event.description}")
        print(f"üí≠ Thought: {event.thought}")

    elif isinstance(event, ScreenshotEvent):
        save_screenshot(event.screenshot, "screenshot.png")

    elif isinstance(event, TaskThinkingEvent):
        print(f"üêç Generated code:")
        if event.code:
            print(event.code)
        if event.thoughts:
            print(f"üí≠ Thoughts: {event.thoughts}")

# Wait for final result
result = await handler
print(f"‚úÖ Success: {result.success}")
print(f"üìù Reason: {result.reason}")
```

## Event Types

### Planning Events

**ManagerContextEvent** - Emitted after Manager prepares context:

```python
class ManagerContextEvent(Event):
    """Signals that Manager has prepared context and is ready for LLM call"""
    pass  # No fields - this is just a signal event
```

**ManagerResponseEvent** - Emitted after Manager receives LLM response:

```python
class ManagerResponseEvent(Event):
    output_planning: str             # Raw validated LLM response from Manager
    usage: Optional[UsageResult]     # Token usage (requests, input/output tokens)
```

**ManagerPlanDetailsEvent** - Emitted when Manager creates/updates a plan:

```python
class ManagerPlanDetailsEvent(Event):
    plan: str              # Full task plan with subgoals
    current_subgoal: str   # Current subgoal being executed
    thought: str           # Manager's reasoning
    manager_answer: str    # Direct answer (if task is complete)
    memory_update: str     # Memory additions from this response
    success: bool | None   # True/False if complete, None if in progress
    full_response: str     # Complete LLM response from Manager
```

### Execution Events

**ExecutorContextEvent** - Emitted after Executor prepares context:

```python
class ExecutorContextEvent(Event):
    messages: list  # ChatMessage list for LLM
    subgoal: str    # Current subgoal being executed
```

**ExecutorResponseEvent** - Emitted after Executor receives LLM response:

```python
class ExecutorResponseEvent(Event):
    response_text: str               # Raw LLM response from Executor
    usage: Optional[UsageResult]     # Token usage (requests, input/output tokens)
```

**ExecutorActionEvent** - Emitted when Executor selects an action:

```python
class ExecutorActionEvent(Event):
    action_json: str    # JSON representation of selected action
    thought: str        # Executor's reasoning
    description: str    # Human-readable action description
    full_response: str  # Complete LLM response from Executor
```

**ExecutorActionResultEvent** - Emitted after Executor action execution:

```python
class ExecutorActionResultEvent(Event):
    action: Dict        # The executed action
    outcome: bool       # Whether action succeeded
    error: str          # Error message (if failed)
    summary: str        # Human-readable summary of result
    thought: str        # Executor's reasoning
    action_json: str    # Raw action JSON
    full_response: str  # Complete LLM response from Executor
```

**TaskThinkingEvent** - Emitted when CodeAct generates code (direct mode):

```python
class TaskThinkingEvent(Event):
    thoughts: Optional[str]          # LLM's reasoning process
    code: Optional[str]              # Generated Python code
    usage: Optional[UsageResult]     # Token usage (requests, input/output tokens)
```

**TaskExecutionResultEvent** - Emitted after code execution:

```python
class TaskExecutionResultEvent(Event):
    output: str      # Execution output or error message
```

### Script Execution Events

**ScripterThinkingEvent** - Emitted when ScripterAgent generates code:

```python
class ScripterThinkingEvent(Event):
    thoughts: str                    # LLM's reasoning process
    code: Optional[str]              # Generated Python code
    full_response: str               # Complete LLM response
    usage: Optional[UsageResult]     # Token usage (requests, input/output tokens)
```

### Visual Events

**ScreenshotEvent** - Emitted when a screenshot is captured:

```python
class ScreenshotEvent(Event):
    screenshot: bytes  # PNG image data
```

**RecordUIStateEvent** - Emitted when UI state is captured:

```python
class RecordUIStateEvent(Event):
    ui_state: list[Dict[str, Any]]  # Accessibility tree as structured data
```

### Usage Tracking

**UsageResult** - Token usage information included in LLM response events:

```python
class UsageResult(BaseModel):
    request_tokens: int   # Number of tokens in the prompt
    response_tokens: int  # Number of tokens in the response
    total_tokens: int     # Total tokens used (request + response)
    requests: int         # Number of LLM requests made
```

## Common Patterns

### Building a Live UI

```python
async def run_with_ui(goal: str):
    agent = DroidAgent(goal=goal, config=config)
    handler = agent.run()

    async for event in handler.stream_events():
        if isinstance(event, ManagerPlanDetailsEvent):
            ui.update_plan(event.plan)
            ui.update_current_step(event.current_subgoal)

        elif isinstance(event, ExecutorActionEvent):
            ui.add_action_log(event.description, event.thought)

        elif isinstance(event, ScreenshotEvent):
            ui.update_screenshot(event.screenshot)

    result = await handler
    ui.show_completion(result.success, result.reason)
```

### Tracking Token Usage

```python
async def track_token_usage(goal: str):
    agent = DroidAgent(goal=goal, config=config)
    handler = agent.run()

    total_tokens = 0
    total_requests = 0

    async for event in handler.stream_events():
        # Check for events that contain usage information
        if hasattr(event, 'usage') and event.usage:
            total_tokens += event.usage.total_tokens
            total_requests += event.usage.requests

            print(f"LLM call - Input: {event.usage.request_tokens}, "
                  f"Output: {event.usage.response_tokens}, "
                  f"Total: {event.usage.total_tokens}")

    result = await handler
    print(f"\nüìä Total tokens used: {total_tokens}")
    print(f"üìä Total LLM requests: {total_requests}")
```

### Logging and Monitoring

```python
import logging

logger = logging.getLogger("droidrun.monitor")

async def monitor_execution(goal: str):
    agent = DroidAgent(goal=goal, config=config)
    handler = agent.run()

    start_time = time.time()
    action_count = 0

    async for event in handler.stream_events():
        if isinstance(event, ExecutorActionEvent):
            action_count += 1
            logger.info(f"Action {action_count}: {event.description}")

        elif isinstance(event, TaskExecutionResultEvent):
            logger.info(f"Code execution result: {event.output}")

    result = await handler
    duration = time.time() - start_time

    logger.info(f"Task completed in {duration:.2f}s with {action_count} actions")
    logger.info(f"Result: {result.success} - {result.reason}")
```

## Notes

- Events are **streamed in real-time** as the agent executes
- Not all events are emitted in every execution (depends on mode and actions)
- **Reasoning mode** emits `ManagerContextEvent`, `ManagerResponseEvent`, `ManagerPlanDetailsEvent`, `ExecutorContextEvent`, `ExecutorResponseEvent`, `ExecutorActionEvent`, and `ExecutorActionResultEvent`
- **Direct mode** emits `TaskThinkingEvent` and `TaskExecutionResultEvent`
- All events are **Pydantic models** with full type safety
- The `handler` object is **async** - always use `await handler` to get the final result

## Learn More

- [LlamaIndex Workflows](https://docs.llamaindex.ai/en/stable/understanding/workflows/) - The underlying orchestration system
